{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install unsloth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Replace 'your_access_token' with your actual token\n",
    "login(token=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"22-24/Final\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "\n",
    "# Load your dataset (replace with the actual dataset name)\n",
    "# dataset = load_dataset('your_dataset_name')\n",
    "\n",
    "# Define a function to resize images\n",
    "def resize_image(example):\n",
    "    # Check if the 'image' is already a PIL Image object or a file path\n",
    "    if isinstance(example['image'], Image.Image):\n",
    "        image = example['image']\n",
    "    else:\n",
    "        image = Image.open(example['image'])  # Adjust according to how your dataset stores images\n",
    "\n",
    "    # Resize to 32x32\n",
    "    image = image.resize((32, 32)).resize((512,512))\n",
    "    example['image'] = image\n",
    "    return example\n",
    "\n",
    "# Apply the resize function to the dataset\n",
    "dataset = dataset.map(resize_image, batched=False)\n",
    "\n",
    "# Check if the resizing was successful\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
    "import torch\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\", # Llama 3.2 vision support\n",
    "    \"unsloth/Llama-3.2-11B-Vision-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit\", # Can fit in a 80GB card!\n",
    "    \"unsloth/Llama-3.2-90B-Vision-bnb-4bit\",\n",
    "\n",
    "    \"unsloth/Pixtral-12B-2409-bnb-4bit\",              # Pixtral fits in 16GB!\n",
    "    \"unsloth/Pixtral-12B-Base-2409-bnb-4bit\",         # Pixtral base model\n",
    "\n",
    "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",          # Qwen2 VL support\n",
    "    \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Qwen2-VL-72B-Instruct-bnb-4bit\",\n",
    "\n",
    "    \"unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit\",      # Any Llava variant works!\n",
    "    \"unsloth/llava-1.5-7b-hf-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"22-24/pixtral_2\",\n",
    "    load_in_4bit = False, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = True, # False if not finetuning vision layers\n",
    "    finetune_language_layers   = True, # False if not finetuning language layers\n",
    "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
    "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
    "\n",
    "    r = 128,           # The larger, the higher the accuracy, but might overfit\n",
    "    lora_alpha = 128,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "instruction = \"You are an expert radiographer. Describe accurately what you see in this image.\"\n",
    "\n",
    "system_message = \"\"\"\n",
    "  Analyse the given AI-generated image for artifacts and report their exact locations in the image. \n",
    "  The image may have more than one artifact. Return all of them, carefully making sure that the perceived artifacts \n",
    "  actually exist, and give reasons for the detection of each artifact. \n",
    "    Return your outputs in the form of a JSON, as follows: \n",
    "    {\n",
    "        \"Artifact Class Name\" : \"reason\",\n",
    "        ...\n",
    "        \"Artifact Class Name\" : \"reason\"\n",
    "    }\n",
    "\n",
    "    Categories of artifacts to check for:\n",
    "    - Ambiguous / color / depth related artifacts:\n",
    "      - Inconsistent object boundaries\n",
    "      - Artificial noise patterns in uniform surfaces\n",
    "      - Unrealistic specular highlights\n",
    "      - Incorrect perspective rendering\n",
    "      - Scale inconsistencies within single objects\n",
    "      - Spatial relationship errors\n",
    "      - Depth perception anomalies\n",
    "      - Artificial enhancement artifacts\n",
    "      - Regular grid-like artifacts in textures\n",
    "      - Repeated element patterns\n",
    "      - Systematic color distribution anomalies\n",
    "      - Frequency domain signatures\n",
    "      - Color coherence breaks\n",
    "      - Unnatural color transitions\n",
    "      - Fake depth of field\n",
    "      - Abruptly cut off objects\n",
    "      - Glow or light bleed around object boundaries\n",
    "      - Ghosting effects: Semi-transparent duplicates of elements\n",
    "      - Cinematization Effects\n",
    "      - Artificial smoothness\n",
    "      - Movie-poster like composition of ordinary scenes\n",
    "      - Artificial depth of field in object presentation\n",
    "      - Scale inconsistencies within the same object class\n",
    "    - Texture related artifacts:\n",
    "      - Texture bleeding between adjacent regions\n",
    "      - Texture repetition patterns\n",
    "      - Over-smoothing of natural textures\n",
    "      - Metallic surface artifacts\n",
    "      - Over-sharpening artifacts\n",
    "      - Aliasing along high-contrast edges\n",
    "      - Blurred boundaries in fine details\n",
    "      - Jagged edges in curved structures\n",
    "      - Loss of fine detail in complex structures\n",
    "      - Random noise patterns in detailed areas\n",
    "      - Resolution inconsistencies within regions\n",
    "      - Synthetic material appearance\n",
    "      - Excessive sharpness in certain image regions\n",
    "    - Mechanical artifacts:\n",
    "      - Physically impossible structural elements\n",
    "      - Implausible aerodynamic structures\n",
    "      - Impossible mechanical joints\n",
    "      - Impossible mechanical connections\n",
    "      - Inconsistent scale of mechanical parts\n",
    "      - Floating or disconnected components\n",
    "      - Asymmetric features in naturally symmetric objects\n",
    "      - Discontinuous surfaces\n",
    "      - Non-manifold geometries in rigid structures\n",
    "      - Irregular proportions in mechanical components\n",
    "      - Inconsistent material properties\n",
    "    - Animals related artifacts:\n",
    "      - Dental anomalies in mammals\n",
    "      - Anatomically incorrect paw structures\n",
    "      - Improper fur direction flows\n",
    "      - Unrealistic eye reflections\n",
    "      - Misshapen ears or appendages\n",
    "      - Anatomically impossible joint configurations\n",
    "      - Unnatural pose artifacts\n",
    "      - Biological asymmetry errors\n",
    "      - Impossible foreshortening in animal bodies\n",
    "      - Misaligned bilateral elements in animal faces\n",
    "      - Incorrect Skin Tones\n",
    "    - Light related artifacts:\n",
    "      - Inconsistent shadow directions\n",
    "      - Multiple light source conflicts\n",
    "      - Missing ambient occlusion\n",
    "      - Incorrect reflection mapping\n",
    "      - Distorted window reflections\n",
    "      - Unnatural Lighting Gradients\n",
    "      - Unnaturally glossy surfaces\n",
    "      - Dramatic lighting that defies natural physics\n",
    "      - Multiple inconsistent shadow sources\n",
    "    - Vehicular artifacts:\n",
    "      - Incorrect wheel geometry\n",
    "      - Misaligned body panels\n",
    "\n",
    "    Analyze the following AI-generated image for artifacts and provide detailed findings.And only give the\n",
    "    answer from the above artifacts only don't make up any from own you are only allowed to select from the above list only \n",
    "    don't come up with anything made up.\n",
    "    \"\"\"\n",
    "def convert_to_conversation(sample):\n",
    "\n",
    "    conversation = [\n",
    "        { \"role\": \"user\",\n",
    "          \"content\" : [\n",
    "            {\"type\" : \"text\",  \"text\"  : system_message},\n",
    "            {\"type\" : \"image\", \"image\" : sample[\"image\"]} ]\n",
    "        },\n",
    "        { \"role\" : \"assistant\",\n",
    "          \"content\" : [\n",
    "            {\"type\" : \"text\",  \"text\"  : sample[\"answer\"]} ]\n",
    "        },\n",
    "    ]\n",
    "    return { \"messages\" : conversation }\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "converted_dataset = [convert_to_conversation(sample) for sample in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Log in to WandB\n",
    "wandb.login(key=\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import is_bf16_supported\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "FastVisionModel.for_training(model) # Enable for training!\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = UnslothVisionDataCollator(model, tokenizer), # Must use!\n",
    "    train_dataset = converted_dataset,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 30,\n",
    "        # num_train_epochs = 1, # Set this instead of max_steps for full training runs\n",
    "        learning_rate = 3e-5,\n",
    "        fp16 = not is_bf16_supported(),\n",
    "        bf16 = is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"LLAMA-COT\",\n",
    "        report_to = \"wandb\",     # For Weights and Biases\n",
    "\n",
    "        # You MUST put the below items for vision finetuning:\n",
    "        remove_unused_columns = False,\n",
    "        dataset_text_field = \"\",\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "        dataset_num_proc = 4,\n",
    "        max_seq_length = 2048,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "FastVisionModel.for_inference(model) # Enable for inference!\n",
    "\n",
    "image = dataset[10][\"image\"]\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": system_message}\n",
    "    ]}\n",
    "]\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n",
    "inputs = tokenizer(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens = False,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model.push_to_hub(\"22-24/llama_32X32\", token = \"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\") # Online saving\n",
    "tokenizer.push_to_hub(\"22-24/llama_32X32\", token = \"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\") # Online saving"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
