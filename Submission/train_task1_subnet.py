import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.models as models
import torchvision.datasets as datasets
from torch.utils.data import DataLoader
import torch.nn.functional as F
import timm
import wandb
import random
import torchvision
from torchvision.transforms import GaussianBlur, ColorJitter
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import torchinfo
import torch.fft
import pywt
import matplotlib.pyplot as plt
from PIL import Image
import torchvision.transforms as T
import os
from PIL import Image
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import io

# Set device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("Using device:", device)


# Hyperparameters and configurations
dataset_dir = ""  # Provide the path for dataset 
img_height, img_width = 32, 32
batch_size = 512
num_epochs = 10  #Epochs
early_stopping_patience = 10
FFT = True   
IN_CHANS=3 if not FFT else 9
RESIZE_HEIGHT = 224
RESIZE_WIDTH = 224
NUM_SAVE_STEPS=100
MAX_TRAIN_STEPS = 600
NUM_EPOCHS = 4
MODEL = "resnetv2_50"   #Model Name

#Defined Arguments 
primary_dataset_path = "/workspace/archive/train"   # Provide Path to training Dataset 
additional_dirs = [
    "/train"            #Additional directories for the training dataset generated by image generation script can be provided here
    ]
WANDB_API_KEY = ""      # Provide the WANDB API KEY
WANDB_PROJECT_NAME = ""     # Set the Project name

# ------------------------------------------ Custom Transforms ------------------------------------------------------------

class RandomApplyGaussianNoise:
    """
    Apply Gaussian noise to an image with a given probability.

    """
    def __init__(self, probability=0.15, mean=0.0, std=0.1):
        self.probability = probability
        self.mean = mean
        self.std = std

    def __call__(self, img):
        if random.random() < self.probability:
            if not isinstance(img, torch.Tensor):
                img = transforms.ToTensor()(img)
            
            noise = torch.randn(img.size(), device=img.device) * self.std + self.mean
            img = img + noise
            img = torch.clamp(img, 0, 1) 
        return img

class RandomApplyGaussianBlur:
    """
    Apply Gaussian blur to an image with a given probability.

    """

    def __init__(self, probability=0.15, kernel_size=(3, 3), sigma=(0.1, 2.0)):
        self.probability = probability
        self.gaussian_blur = transforms.GaussianBlur(kernel_size=kernel_size, sigma=sigma)

    def __call__(self, img):
        if random.random() < self.probability:
            return self.gaussian_blur(img)
        return img

class RandomApplyColorJitter:
    """
    Apply color jitter to an image with a given probability.

    """
    def __init__(self, probability=0.15, brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1):
        self.probability = probability
        self.color_jitter = ColorJitter(brightness=brightness, contrast=contrast, saturation=saturation, hue=hue)

    def __call__(self, img):
        if random.random() < self.probability:
            return self.color_jitter(img)
        return img

class RandomRotate:
    """
    Apply random rotation to an image with a given probability.

    """
    def __init__(self, probability=0.15, degrees=20):
        self.probability = probability
        self.rotation = transforms.RandomRotation(degrees=degrees)

    def __call__(self, img):
        if random.random() < self.probability:
            return self.rotation(img)
        return img

# ------------------------------------------ Feature Extraction ------------------------------------------------------------

def load_image(image_path):
    """
    Load an image and apply basic transformations.

    Parameters
    ----------
    image_path : str (Path to the input image)

    Returns
    -------
    torch.Tensor (Transformed image tensor of shape (1, 3, 32, 32).)

    Modifies
    --------
    None
    """
    transform = T.Compose([

        T.Resize((32, 32)),
        T.ToTensor()
    ])
    image = Image.open(image_path).convert("RGB")  
    return transform(image).unsqueeze(0)  

class FeatureExtractor(nn.Module):
    """
    Feature extraction using FFT and Wavelet transforms.

    Attributes
    ----------
    low_pass_radius : int (Radius for low-pass filter in FFT.)
    resolution : int (Resolution to which the features will be interpolated.)

    Methods
    -------
    apply_low_pass_filter(fft_complex)
        Applies a low-pass filter to the FFT-transformed input.

    apply_high_pass_filter(fft_complex)
        Applies a high-pass filter to the FFT-transformed input.

    extract_fft_features(x)
        Extracts FFT features using low-pass and high-pass filters.

    extract_wavelet_features(x)
        Extracts Wavelet features from the input tensor.

    forward(x)
        Combines FFT and Wavelet features with the input tensor.
    """
    def __init__(self, low_pass_radius=4,resolution=224):
        """
        Initialize the FeatureExtractor.

        Parameters
        ----------
        low_pass_radius : int, optional      
        resolution : int, optional

        """
        super(FeatureExtractor, self).__init__()
        self.low_pass_radius = low_pass_radius
        self.res = resolution

    def apply_low_pass_filter(self, fft_complex):
        """
        Apply a low-pass filter to the FFT-transformed input.

        Parameters
        ----------
        fft_complex : torch.Tensor
            Complex tensor of FFT-transformed input of shape (B, C, H, W).

        Returns
        -------
        torch.Tensor
            Low-pass filtered tensor of shape (B, C, H, W).
        """
        
        B, C, H, W = fft_complex.shape
        mask = torch.zeros(H, W)
        center_x, center_y = H // 2, W // 2

        for x in range(H):
            for y in range(W):
                if np.sqrt((x - center_x)**2 + (y - center_y)**2) < self.low_pass_radius:
                    mask[x, y] = 1

        mask = mask.to(fft_complex.device).unsqueeze(0).unsqueeze(0)
        mask = mask.expand(B, C, H, W)
        return fft_complex * mask

    def apply_high_pass_filter(self, fft_complex):
        """
        Apply a high-pass filter to the FFT-transformed input.

        Parameters
        ----------
        fft_complex : torch.Tensor
            Complex tensor of FFT-transformed input of shape (B, C, H, W).

        Returns
        -------
        torch.Tensor
            High-pass filtered tensor of shape (B, C, H, W).
        """
        B, C, H, W = fft_complex.shape
        mask = torch.ones(H, W)
        center_x, center_y = H // 2, W // 2

        for x in range(H):
            for y in range(W):
                if np.sqrt((x - center_x)**2 + (y - center_y)**2) < self.low_pass_radius:
                    mask[x, y] = 0

        mask = mask.to(fft_complex.device).unsqueeze(0).unsqueeze(0)
        mask = mask.expand(B, C, H, W)
        return fft_complex * mask

    def extract_fft_features(self, x):
        """
        Extract FFT features using low-pass and high-pass filters.

        Parameters
        ----------
        x : torch.Tensor
            Input tensor of shape (B, C, H, W).

        Returns
        -------
        torch.Tensor
            FFT features tensor of shape (B, 6, H, W).
        """
        fft_complex = torch.fft.fft2(x)
        fft_complex_shifted = torch.fft.fftshift(fft_complex)

        
        low_pass = self.apply_low_pass_filter(fft_complex_shifted)
        high_pass = self.apply_high_pass_filter(fft_complex_shifted)

        
        low_pass_ifft = torch.fft.ifft2(torch.fft.ifftshift(low_pass)).abs()
        high_pass_ifft = torch.fft.ifft2(torch.fft.ifftshift(high_pass)).abs()

        
        low_pass_magnitude = torch.abs(low_pass_ifft)
        high_pass_magnitude = torch.abs(high_pass_ifft)

        
        low_pass_magnitude = low_pass_magnitude / torch.max(low_pass_magnitude)
        high_pass_magnitude = high_pass_magnitude / torch.max(high_pass_magnitude)

       
        fft_features = torch.cat([low_pass_magnitude, high_pass_magnitude], dim=1)  # Shape: (B, 6, 32, 32)
        return fft_features

    def extract_wavelet_features(self, x):
        """
        Extract Wavelet features using Discrete Wavelet Transform.

        Parameters
        ----------
        x : torch.Tensor
            Input tensor of shape (B, C, H, W).

        Returns
        -------
        torch.Tensor
            Wavelet features tensor of shape (B, C*4, resolution, resolution).
        """
        batch_size, channels, height, width = x.size()
        wavelet_features = []

        for b in range(batch_size):
            for c in range(channels):
                img = x[b, c].cpu().detach().numpy()
                coeffs = pywt.dwt2(img, 'haar')
                cA, (cH, cV, cD) = coeffs

                
                cA = torch.tensor(cA).float()
                cH = torch.tensor(cH).float()
                cV = torch.tensor(cV).float()
                cD = torch.tensor(cD).float()

                cA = nn.functional.interpolate(cA.unsqueeze(0).unsqueeze(0), size=(self.res,self.res), mode='bilinear').squeeze()
                cH = nn.functional.interpolate(cH.unsqueeze(0).unsqueeze(0), size=(self.res,self.res), mode='bilinear').squeeze()
                cV = nn.functional.interpolate(cV.unsqueeze(0).unsqueeze(0), size=(self.res,self.res), mode='bilinear').squeeze()
                cD = nn.functional.interpolate(cD.unsqueeze(0).unsqueeze(0), size=(self.res,self.res), mode='bilinear').squeeze()

                wavelet_features.extend([cA, cH, cV, cD])

        wavelet_features = torch.stack(wavelet_features).view(batch_size, channels * 4, height, width)
        return wavelet_features

    def forward(self, x):
        """
        Perform a forward pass to combine original, and  FFT features.

        Parameters
        ----------
        x : torch.Tensor
            Input tensor of shape (B, C, H, W).

        Returns
        -------
        torch.Tensor
            Combined features tensor of shape (B, C+6, H, W).
        """
        fft_features = self.extract_fft_features(x)      
        # wavelet_features = self.extract_wavelet_features(x)  
        combined_features = torch.cat((x, fft_features.to(x.device,x.dtype)), dim=1)
        return combined_features


class CombinedDataset(Dataset):
    """
    Custom dataset combining primary dataset and additional directories.

    Attributes
    ----------
    transform : callable Transformations to apply to the images.
    data : list of tuples (image_path, label, is_png) for all dataset images.
     
    Methods
    -------
    __len__() 
    __getitem__(idx) 
        
    """
    def __init__(self, primary_dataset_path, additional_dirs, transform=None):
        """
        Args:
            primary_dataset_path (str): Path to the main dataset directory.
            additional_dirs (list): List of paths to additional directories.
            transform (callable, optional): Transformations to apply to the images.
        """
        self.transform = transform
        self.data = []  

        
        real_dir = os.path.join(primary_dataset_path, "REAL")
        fake_dir = os.path.join(primary_dataset_path, "FAKE")

        self.data += [(os.path.join(real_dir, img), 0, False) for img in os.listdir(real_dir) if img.endswith(".jpg")]
        self.data += [(os.path.join(real_dir, img), 0, False) for img in os.listdir(real_dir) if img.endswith(".jpg")]
        self.data += [(os.path.join(fake_dir, img), 1, False) for img in os.listdir(fake_dir) if img.endswith(".jpg")]

        
        for directory in additional_dirs:
            for folder in os.listdir(directory):
                folder_path = os.path.join(directory, folder)
                if os.path.isdir(folder_path):
                    for img in os.listdir(folder_path):
                        if img.endswith(".png"):
                            self.data.append((os.path.join(folder_path, img), 1, True)) 

    def __len__(self):
        """
        Get the total number of samples in the dataset.

        Returns
        -------
        int
            Number of samples in the dataset.
        """
        return len(self.data)

    def __getitem__(self, idx):
        """
        Retrieve an image and its label by index.

        Parameters
        ----------
        idx : int
            Index of the sample.

        Returns
        -------
        tuple
            (transformed_image, label), where label is 0 or 1.
        """
        img_path, label, is_png = self.data[idx]
        image = Image.open(img_path).convert("RGB").resize((32,32),Image.BILINEAR)

        if is_png:
            with io.BytesIO() as buffer:
                image.save(buffer, format="JPEG")
                buffer.seek(0)
                image = Image.open(buffer)
                image.load()
        
        if self.transform:
            image = self.transform(image)

        return image, label


class Classifier(nn.Module):
    """
    Neural network classifier with backbone and custom head.

    Attributes
    ----------
    bb : timm.models.features_only
        Backbone feature extractor from timm.
    head : nn.Sequential
        Custom head for classification.

    Methods
    -------
    forward(x, temperature=1)
        Perform a forward pass and return predictions.
    """
    def __init__(self):
        """
        Initialize the classifier with a backbone model.

        Parameters
        ----------
        model : str
            Model name from timm library.
        """
        super().__init__()
        self.bb = timm.create_model(MODEL,pretrained=True,in_chans=IN_CHANS,features_only=True)
        self.head = nn.Sequential(
            nn.Conv2d(2048,32,3,2),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(32*9,32),
            nn.ReLU(),
            nn.Linear(32,1),

        )
        self.bb.requires_grad_ = True
        self.bb.train()
        self.head.requires_grad_ = True

    def forward(self,x):
        '''
        Perform a forward pass.

        Parameters
        ----------
        x : torch.Tensor
            Input tensor of shape (B, C, H, W).

        Returns
        -------
        tuple
            - torch.Tensor: Predictions of shape (B, 1).
            - list: Intermediate feature maps.
        '''
        features = self.bb(x)

        out = F.sigmoid(self.head(features[-1]))
        return out,features

# class SubNetwork(nn.Module):
#     def __init__(self):
#         super().__init__()
#         self.ad_0 = 

# ------------------------------------------ MAIN ------------------------------------------------------------

data_transforms = transforms.Compose([
    transforms.Resize((RESIZE_HEIGHT, RESIZE_WIDTH)),
    transforms.ToTensor(),
    RandomApplyGaussianNoise(probability=0.23),
    RandomApplyGaussianBlur(probability=0.23),
    RandomApplyColorJitter(probability=0.23),
    transforms.RandomHorizontalFlip(p=0.43),
    RandomRotate(0.2,20),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),
])
combined_dataset = CombinedDataset(primary_dataset_path, additional_dirs, transform=data_transforms)


dataloader = DataLoader(combined_dataset, batch_size=256, shuffle=True,num_workers=8)


model = Classifier().to("cuda")
fft_extractor = FeatureExtractor(resolution=224,low_pass_radius=2)
torchinfo.summary(model.bb)
torchinfo.summary(model.head)

optimizer = optim.AdamW(model.parameters(),lr=8e-5,weight_decay=0.005)
# lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,"min",0.85,patience=10)

# Training loop
wandb.login(key=WANDB_API_KEY)
wandb.init(project=WANDB_PROJECT_NAME)
for epoch in range(NUM_EPOCHS):
    print(epoch, "Epoch")
    for i,batch in enumerate(dataloader):
        imgs, labels = batch
        labels = labels.float().unsqueeze(-1).to("cuda")
        if FFT:
            imgs = fft_extractor(imgs.to("cuda"))
        else:
            imgs = imgs.to("cuda")
        preds,feats = model(imgs)
        loss = F.binary_cross_entropy(preds,labels)
        optimizer.zero_grad()
        loss.backward()
        wandb.log({"loss":loss})
        optimizer.step()
        # lr_scheduler.step(loss)

        if i%10==0:
            print(f"Loss: {loss}, Step: {i}")
        if i%NUM_SAVE_STEPS==0:
            if FFT:
                os.makedirs(f"ckpts/{MODEL}_fft",exist_ok=True)
            else:
                os.makedirs(f"ckpts/{MODEL}",exist_ok=True)
            if FFT:
                torch.save(model.state_dict(),f"ckpts/{MODEL}_fft/{i}.pt")
            else:
                torch.save(model.state_dict(),f"ckpts/{MODEL}/{i}.pt")
 

    if FFT:

        torch.save(model.state_dict(),f"ckpts/{MODEL}_fft/epoch_{epoch}.pt")
    else:
        torch.save(model.state_dict(),f"ckpts/{MODEL}/epoch_{epoch}.pt")

wandb.finish()





        




