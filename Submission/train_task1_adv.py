import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.models as models
import torchvision.datasets as datasets
from torch.utils.data import DataLoader
import torch.nn.functional as F
import os
import timm
import wandb
import torchinfo
import random
import torchvision
from torchvision.transforms import GaussianBlur, ColorJitter
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import torch.fft
import pywt
from PIL import Image
import torchvision.transforms as T
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import io
""""""""

# Set device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# Hyperparameters and configurations
BATCH_SIZE = 256
FFT = False 
IN_CHANS=3 if not FFT else 9
LR = 1e-3   
WEIGHT_DECAY = 1e-4
RESIZE_HEIGHT = 224
RESIZE_WIDTH = 224
NUM_SAVE_STEPS=100
NUM_EPOCHS = 4
MODEL = "tf_efficientnet_b3"  #Model Name
TRAIN_TEMP = 60

PGD_EPSILON_LIM = 0.1
PGD_ALPHA_LIM = 0.05
PGD_MAX_ITER = 25

#Defined Arguments 
primary_dataset_path = "/workspace/archive/train" # Provide the path to geerated dataset according to the Image generation script
additional_dirs = [
    "/train"            #Additional directories for the training dataset generated by image generation script can be provided here
    ]       
WANDB_API_KEY = ""  # Provide the WANDB API KEY
WANDB_PROJECT_NAME = "" #Set the Project Name 

# ------------------------------------------ Custom Transforms ------------------------------------------------------------
class RandomApplyGaussianNoise:
    """
    Apply Gaussian noise to an image with a given probability.

    """
    def __init__(self, probability=0.15, mean=0.0, std=0.2):
        self.probability = probability
        self.mean = mean
        self.std = std

    def __call__(self, img):
        if random.random() < self.probability:
            
            if not isinstance(img, torch.Tensor):
                img = transforms.ToTensor()(img)
            
            
            noise = torch.randn(img.size(), device=img.device) * self.std + self.mean
            img = img + noise
            img = torch.clamp(img, 0, 1)  
        return img

class RandomApplyGaussianBlur:
    """
    Apply Gaussian blur to an image with a given probability.

    """
    def __init__(self, probability=0.15, kernel_size=(7, 7), sigma=(0.1, 2.0)):
        self.probability = probability
        self.gaussian_blur = transforms.GaussianBlur(kernel_size=kernel_size, sigma=sigma)

    def __call__(self, img):
        if random.random() < self.probability:
            return self.gaussian_blur(img)
        return img

class RandomApplyColorJitter:
    """
    Apply color jitter to an image with a given probability.

    """
    def __init__(self, probability=0.15, brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1):
        self.probability = probability
        self.color_jitter = ColorJitter(brightness=brightness, contrast=contrast, saturation=saturation, hue=hue)

    def __call__(self, img):
        if random.random() < self.probability:
            return self.color_jitter(img)
        return img

class RandomRotate:
    """
    Apply random rotation to an image with a given probability.

    """
    def __init__(self, probability=0.15, degrees=20):
        self.probability = probability
        self.rotation = transforms.RandomRotation(degrees=degrees)
    def __call__(self, img):
        if random.random() < self.probability:
            return self.rotation(img)
        return img

#-------------------------------------------------ATTACK-----------------------------------------------------------------

def apply_pgd(model, images, labels, epsilon=0.1, alpha=0.01, iterations=15):
    """
    Applies Projected Gradient Descent (PGD) to generate adversarial images.

    Parameters
    ----------
    model : nn.Module
        The model to be attacked.
    images : torch.Tensor
        Original input images of shape (B, C, H, W).
    labels : torch.Tensor
        True labels for the input images of shape (B, 1).
    epsilon : float, optional
        Maximum perturbation allowed (default: 0.1).
    alpha : float, optional
        Step size for each iteration (default: 0.01).
    iterations : int, optional
        Number of PGD iterations (default: 15).

    Returns
    -------
    torch.Tensor
        Adversarial images of shape (B, C, H, W).
    """

    alpha = alpha.view(alpha.shape[0],1,1,1)

    model.eval()
    model.requires_grad = False

    adversarial_images = images.clone().detach().requires_grad_(True)

    for _ in range(iterations):

        outputs = model(adversarial_images,TRAIN_TEMP)

        loss = F.binary_cross_entropy(outputs, labels)

        model.zero_grad()
        loss.backward()

        gradient_sign = adversarial_images.grad.data.sign()
        adversarial_images = adversarial_images + alpha * gradient_sign

        perturbation = torch.clamp(adversarial_images - images, -epsilon, epsilon)
        adversarial_images = torch.clamp(images + perturbation, 0, 1).detach().requires_grad_(True)

    adversarial_images.requires_grad_(False)
    model.train()
    model.zero_grad()
    model.requires_grad = True

    return adversarial_images

# ------------------------------------------ Feature Extraction ------------------------------------------------------------

def load_image(image_path):
    """
    Load an image and apply basic transformations.

    Parameters
    ----------
    image_path : str (Path to the input image)

    Returns
    -------
    torch.Tensor (Transformed image tensor of shape (1, 3, 32, 32).)

    Modifies
    --------
    None
    """
    transform = T.Compose([

        T.Resize((32, 32)),
        T.ToTensor()
    ])
    image = Image.open(image_path).convert("RGB")  
    return transform(image).unsqueeze(0)  


class FeatureExtractor(nn.Module):
    """
    Feature extraction using FFT and Wavelet transforms.

    Attributes
    ----------
    low_pass_radius : int (Radius for low-pass filter in FFT.)
    resolution : int (Resolution to which the features will be interpolated.)

    Methods
    -------
    apply_low_pass_filter(fft_complex)
        Applies a low-pass filter to the FFT-transformed input.

    apply_high_pass_filter(fft_complex)
        Applies a high-pass filter to the FFT-transformed input.

    extract_fft_features(x)
        Extracts FFT features using low-pass and high-pass filters.

    extract_wavelet_features(x)
        Extracts Wavelet features from the input tensor.

    forward(x)
        Combines FFT and Wavelet features with the input tensor.
    """
    def __init__(self, low_pass_radius=4,resolution=224):
        """
        Initialize the FeatureExtractor.

        Parameters
        ----------
        low_pass_radius : int, optional      
        resolution : int, optional

        """
        super(FeatureExtractor, self).__init__()
        self.low_pass_radius = low_pass_radius
        self.res = resolution

    def apply_low_pass_filter(self, fft_complex):
        """
        Apply a low-pass filter to the FFT-transformed input.

        Parameters
        ----------
        fft_complex : torch.Tensor
            Complex tensor of FFT-transformed input of shape (B, C, H, W).

        Returns
        -------
        torch.Tensor
            Low-pass filtered tensor of shape (B, C, H, W).
        """
        
        B, C, H, W = fft_complex.shape
        mask = torch.zeros(H, W)
        center_x, center_y = H // 2, W // 2

        for x in range(H):
            for y in range(W):
                if np.sqrt((x - center_x)**2 + (y - center_y)**2) < self.low_pass_radius:
                    mask[x, y] = 1

        mask = mask.to(fft_complex.device).unsqueeze(0).unsqueeze(0)
        mask = mask.expand(B, C, H, W)
        return fft_complex * mask

    def apply_high_pass_filter(self, fft_complex):
        """
        Apply a high-pass filter to the FFT-transformed input.

        Parameters
        ----------
        fft_complex : torch.Tensor
            Complex tensor of FFT-transformed input of shape (B, C, H, W).

        Returns
        -------
        torch.Tensor
            High-pass filtered tensor of shape (B, C, H, W).
        """
        B, C, H, W = fft_complex.shape
        mask = torch.ones(H, W)
        center_x, center_y = H // 2, W // 2

        for x in range(H):
            for y in range(W):
                if np.sqrt((x - center_x)**2 + (y - center_y)**2) < self.low_pass_radius:
                    mask[x, y] = 0

        mask = mask.to(fft_complex.device).unsqueeze(0).unsqueeze(0)
        mask = mask.expand(B, C, H, W)
        return fft_complex * mask

    def extract_fft_features(self, x):
        """
        Extract FFT features using low-pass and high-pass filters.

        Parameters
        ----------
        x : torch.Tensor
            Input tensor of shape (B, C, H, W).

        Returns
        -------
        torch.Tensor
            FFT features tensor of shape (B, 6, H, W).
        """
        fft_complex = torch.fft.fft2(x)
        fft_complex_shifted = torch.fft.fftshift(fft_complex)

        
        low_pass = self.apply_low_pass_filter(fft_complex_shifted)
        high_pass = self.apply_high_pass_filter(fft_complex_shifted)

        
        low_pass_ifft = torch.fft.ifft2(torch.fft.ifftshift(low_pass)).abs()
        high_pass_ifft = torch.fft.ifft2(torch.fft.ifftshift(high_pass)).abs()

        
        low_pass_magnitude = torch.abs(low_pass_ifft)
        high_pass_magnitude = torch.abs(high_pass_ifft)

        
        low_pass_magnitude = low_pass_magnitude / torch.max(low_pass_magnitude)
        high_pass_magnitude = high_pass_magnitude / torch.max(high_pass_magnitude)

        
        fft_features = torch.cat([low_pass_magnitude, high_pass_magnitude], dim=1)  # Shape: (B, 6, 32, 32)
        return fft_features

    def extract_wavelet_features(self, x):
        """
        Extract Wavelet features using Discrete Wavelet Transform.

        Parameters
        ----------
        x : torch.Tensor
            Input tensor of shape (B, C, H, W).

        Returns
        -------
        torch.Tensor
            Wavelet features tensor of shape (B, C*4, resolution, resolution).
        """
        batch_size, channels, height, width = x.size()
        wavelet_features = []

        for b in range(batch_size):
            for c in range(channels):
                img = x[b, c].cpu().detach().numpy()
                coeffs = pywt.dwt2(img, 'haar')
                cA, (cH, cV, cD) = coeffs

                # Resize components
                cA = torch.tensor(cA).float()
                cH = torch.tensor(cH).float()
                cV = torch.tensor(cV).float()
                cD = torch.tensor(cD).float()

                cA = nn.functional.interpolate(cA.unsqueeze(0).unsqueeze(0), size=(self.res,self.res), mode='bilinear').squeeze()
                cH = nn.functional.interpolate(cH.unsqueeze(0).unsqueeze(0), size=(self.res,self.res), mode='bilinear').squeeze()
                cV = nn.functional.interpolate(cV.unsqueeze(0).unsqueeze(0), size=(self.res,self.res), mode='bilinear').squeeze()
                cD = nn.functional.interpolate(cD.unsqueeze(0).unsqueeze(0), size=(self.res,self.res), mode='bilinear').squeeze()

                wavelet_features.extend([cA, cH, cV, cD])

        wavelet_features = torch.stack(wavelet_features).view(batch_size, channels * 4, height, width)
        return wavelet_features

    def forward(self, x):
        """
        Perform a forward pass to combine original, and  FFT features.

        Parameters
        ----------
        x : torch.Tensor
            Input tensor of shape (B, C, H, W).

        Returns
        -------
        torch.Tensor
            Combined features tensor of shape (B, C+6, H, W).
        """
        fft_features = self.extract_fft_features(x)       
        # wavelet_features = self.extract_wavelet_features(x)  
        combined_features = torch.cat((x, fft_features.to(x.device,x.dtype)), dim=1)
        return combined_features



class CombinedDataset(Dataset):
    """
    Custom dataset combining primary dataset and additional directories.

    Attributes
    ----------
    transform : callable Transformations to apply to the images.
    data : list of tuples (image_path, label, is_png) for all dataset images.
     
    Methods
    -------
    __len__() 
    __getitem__(idx) 
        
    """
    def __init__(self, primary_dataset_path, additional_dirs, transform=None):
        """
        Args:
            primary_dataset_path (str): Path to the main dataset directory.
            additional_dirs (list): List of paths to additional directories.
            transform (callable, optional): Transformations to apply to the images.
        """
        self.transform = transform
        self.data = []  

        
        real_dir = os.path.join(primary_dataset_path, "REAL")
        fake_dir = os.path.join(primary_dataset_path, "FAKE")

        self.data += [(os.path.join(real_dir, img), 0, False) for img in os.listdir(real_dir) if img.endswith(".jpg")]
        self.data += [(os.path.join(real_dir, img), 0, False) for img in os.listdir(real_dir) if img.endswith(".jpg")]
        self.data += [(os.path.join(fake_dir, img), 1, False) for img in os.listdir(fake_dir) if img.endswith(".jpg")]

        
        for directory in additional_dirs:
            for folder in os.listdir(directory):
                folder_path = os.path.join(directory, folder)
                if os.path.isdir(folder_path):
                    for img in os.listdir(folder_path):
                        if img.endswith(".png"):
                            self.data.append((os.path.join(folder_path, img), 1, True)) 

    def __len__(self):
        """
        Get the total number of samples in the dataset.

        Returns
        -------
        int
            Number of samples in the dataset.
        """
        return len(self.data)

    def __getitem__(self, idx):
        """
        Retrieve an image and its label by index.

        Parameters
        ----------
        idx : int
            Index of the sample.

        Returns
        -------
        tuple
            (transformed_image, label), where label is 0 or 1.
        """
        img_path, label, is_png = self.data[idx]
        image = Image.open(img_path).convert("RGB").resize((32,32),Image.BILINEAR)

        if is_png:
            with io.BytesIO() as buffer:
                image.save(buffer, format="JPEG")
                buffer.seek(0)
                image = Image.open(buffer)
                image.load()
        
        if self.transform:
            image = self.transform(image)

        return image, label


class Classifier(nn.Module):
    """
    Neural network classifier with backbone and custom head.

    Attributes
    ----------
    bb : timm.models.features_only
        Backbone feature extractor from timm.
    head : nn.Sequential
        Custom head for classification.

    Methods
    -------
    forward(x, temperature=1)
        Perform a forward pass and return predictions.
    """
    def __init__(self,model):
        """
        Initialize the classifier with a backbone model.

        Parameters
        ----------
        model : str
            Model name from timm library.
        """
        super().__init__()
        self.bb = timm.create_model(model,pretrained=False,in_chans=IN_CHANS,features_only=True)
        if model.startswith("resnet50") or model.startswith("resnetv2_50") or model.startswith("resnext50_32x4d"):
            self.head = nn.Sequential(
                nn.Conv2d(2048,32,3,2),
                nn.ReLU(),
                nn.Flatten(),
                nn.Linear(32*9,32),
                nn.ReLU(),
                nn.Linear(32,1),

            )
        elif model.startswith("mobilenetv4_hybrid_large_075"):
            self.head = nn.Sequential(
                nn.Conv2d(720,32,3,2),
                nn.ReLU(),
                nn.Flatten(),
                nn.Linear(32*9,32),
                nn.ReLU(),
                nn.Linear(32,1),

            )
        elif model.startswith("swinv2_cr_tiny_ns_224"):
            self.head = nn.Sequential(
                nn.Conv2d(768,32,3,2),
                nn.ReLU(),
                nn.Flatten(),
                nn.Linear(32*9,32),
                nn.ReLU(),
                nn.Linear(32,1),

            )
        elif model.startswith("tf_efficientnet_b4"):
            self.head = nn.Sequential(
                nn.Conv2d(448,128,3,2),
                nn.ReLU(),
                nn.Flatten(),
                nn.Linear(128*9,32),
                nn.ReLU(),
                nn.Linear(32,1),

            )

        elif model.startswith("tf_efficientnet_b3"):
            self.head = nn.Sequential(
                nn.Conv2d(384,128,3,2),
                nn.ReLU(),
                nn.Flatten(),
                nn.Linear(128*9,32),
                nn.ReLU(),
                nn.Linear(32,1),

            )
        else:
            self.head = nn.Sequential(
                nn.Conv2d(512,128,3,2),
                nn.ReLU(),
                nn.Flatten(),
                nn.Linear(128*9,32),
                nn.ReLU(),
                nn.Linear(32,1),

            )

            
        self.bb.requires_grad_ = True
        self.bb.train()
        self.head.requires_grad_ = True

    def forward(self,x,temperature = 1):
        """
        Perform a forward pass and return predictions.

        Parameters
        ----------
        x : torch.Tensor
            Input tensor of shape (B, C, H, W).
        temperature : float, optional
            Temperature scaling for the logits (default is 1).

        Returns
        -------
        torch.Tensor
            Sigmoid-activated predictions of shape (B, 1).
        """
        features = self.bb(x)

        out = F.sigmoid(self.head(features[-1])/temperature)
        return out
#--------------------------------------------------------------MAIN-----------------------------------------------


data_transforms = transforms.Compose([
    transforms.Resize((RESIZE_HEIGHT, RESIZE_WIDTH)),
    transforms.ToTensor(),
    RandomApplyGaussianNoise(probability=0.23),
    RandomApplyGaussianBlur(probability=0.23),
    RandomApplyColorJitter(probability=0.23),
    transforms.RandomHorizontalFlip(p=0.5),
    RandomRotate(0.2,20),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),
])


combined_dataset = CombinedDataset(primary_dataset_path, additional_dirs, transform=data_transforms)
dataloader = DataLoader(combined_dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=8)


# Model initialization
model = Classifier(MODEL).to("cuda")
model.load_state_dict(torch.load("ckpts/tf_efficientnet_b4_100/epoch_3.pt",weights_only=True))
fft_extractor = FeatureExtractor(resolution=224,low_pass_radius=4)
torchinfo.summary(model.bb)
torchinfo.summary(model.head)

optimizer = optim.AdamW(model.parameters(),lr=LR,weight_decay=WEIGHT_DECAY)
# lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,"min",0.85,patience=10)

# Training loop
wandb.login(key=WANDB_API_KEY)
wandb.init(project=WANDB_PROJECT_NAME)
for epoch in range(NUM_EPOCHS):
    print(epoch, "Epoch")
    for i,batch in enumerate(dataloader):
        imgs, labels = batch
        labels = labels.float().unsqueeze(-1).to("cuda")
        if FFT:
            imgs = fft_extractor(imgs.to("cuda"))
        else:
            imgs = imgs.to("cuda")

        imgs_clean, imgs_adv = torch.chunk(imgs,2,0)
        labels_clean, labels_adv = torch.chunk(labels,2,0)

        alphas = torch.rand(BATCH_SIZE//2,device=device,dtype=imgs.dtype)*PGD_ALPHA_LIM
        epsilon = random.random()*PGD_EPSILON_LIM


        imgs_adv = apply_pgd(model,imgs_adv,labels_adv,epsilon,alphas,15).clone().detach()


        preds = model(imgs_clean,TRAIN_TEMP)
        preds_adv = model(imgs_adv)
        loss = F.binary_cross_entropy(preds,labels_clean)

        adv_ex_loss = F.binary_cross_entropy(preds_adv,labels_adv)

        final_loss = loss + adv_ex_loss
        optimizer.zero_grad()
        final_loss.backward()
        wandb.log({"loss":loss.item(),"adv_ex_loss":adv_ex_loss.item()})
        optimizer.step()
        # lr_scheduler.step(loss)

        if i%10==0:
            print(f"Loss: {loss}, Step: {i}")
        if i%NUM_SAVE_STEPS==0:
            if FFT:
                os.makedirs(f"ckpts/{MODEL}_fft_{TRAIN_TEMP}_ADV",exist_ok=True)
            else:
                os.makedirs(f"ckpts/{MODEL}_{TRAIN_TEMP}_ADV",exist_ok=True)
            if FFT:
                torch.save(model.state_dict(),f"ckpts/{MODEL}_fft_{TRAIN_TEMP}_ADV/{i}.pt")
            else:
                torch.save(model.state_dict(),f"ckpts/{MODEL}_{TRAIN_TEMP}_ADV/{i}.pt")

    if FFT:

        torch.save(model.state_dict(),f"ckpts/{MODEL}_fft_{TRAIN_TEMP}_ADV/epoch_{epoch}.pt")
    else:
        torch.save(model.state_dict(),f"ckpts/{MODEL}_{TRAIN_TEMP}_ADV/epoch_{epoch}.pt")

wandb.finish()





        




